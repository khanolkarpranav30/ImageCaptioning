{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee95da01-9c15-4744-9405-828e342a017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import requests\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Load the pretrained processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "#Note: You can use the Blip2 model, which is a more powerful pre-trained model for image captioning. \n",
    "#In fact, you can easily incorporate any new pre-trained model that becomes available, as they are continuously developed to be more powerful.  \n",
    "# However, please be aware that the Blip2 model requires 10GB of space, model \n",
    "\n",
    "#Here is the link to the documentation for Blip2: https://huggingface.co/docs/transformers/main/model_doc/blip-2\n",
    "'''\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration #Blip2 models\n",
    "# Load the pretrained processor and model\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736f4375-e8cc-41ee-8ae3-92916a8060e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function captions one image used as an input to a User Interface\n",
    "def caption_image(input_image: np.ndarray):\n",
    "    # Convert numpy array to PIL Image and convert to RGB\n",
    "    raw_image = Image.fromarray(input_image).convert('RGB')\n",
    "    \n",
    "    # Process the image\n",
    "    text = \"the image of\"\n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate a caption for the image\n",
    "    outputs = model.generate(**inputs, max_length=50)\n",
    "\n",
    "    # Decode the generated tokens to text and store it into `caption`\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70fdd526-33c7-4642-b123-763679e420fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_gradio_interface():\n",
    "    iface = gr.Interface(\n",
    "    fn=caption_image, \n",
    "    inputs=gr.Image(), \n",
    "    outputs=\"text\",\n",
    "    title=\"Image Captioning\",\n",
    "    description=\"This is a simple web app for generating captions for images using a trained model.\"\n",
    "    )\n",
    "    iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86b5b501-6664-4b28-a4d4-4c10e266b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code function is used to take a directory as input, and provide captions of all images in that directory\n",
    "def auto_caption_dir_images(image_dir):\n",
    "    image_exts = [\"jpg\", \"jpeg\", \"png\"]\n",
    "    # Open a file to write the captions\n",
    "    N = 0\n",
    "    data = []\n",
    "    with open(\"captions.txt\", \"w\") as caption_file:\n",
    "        # Iterate over each image file in the directory\n",
    "        for ext in image_exts:\n",
    "            # Match both lowercase & uppercase extensions\n",
    "            pattern = os.path.join(image_dir, f\"**/*.{ext}\")\n",
    "            pattern_upper = os.path.join(image_dir, f\"**/*.{ext.upper()}\")\n",
    "            \n",
    "            for img_path in glob.glob(pattern, recursive=True) + glob.glob(pattern_upper, recursive=True):\n",
    "    \n",
    "                print(\"Processing:\", img_path)\n",
    "                \n",
    "                raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "                inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "                out = model.generate(**inputs, max_new_tokens=50)\n",
    "    \n",
    "                caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "                caption_file.write(f\"{os.path.basename(img_path)}: {caption}\\n\")\n",
    "                N = N+1\n",
    "\n",
    "                data.append({\"filename\": str(os.path.basename(img_path)), \"extension\": str(ext),\"caption\": caption})\n",
    "    \n",
    "    print(f\"Caption text file has been generated for {N} images\")\n",
    "\n",
    "    output_path = os.path.join(image_dir, \"Caption_df.xlsx\")\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(output_path, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d535de65-774d-4cdd-bbbc-d0a2de0a78e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The following code launches the Gradio interface, where you can upload the image and obtain its caption\n",
    "launch_gradio_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ac275ae-45bf-44e5-a799-104236444df9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The following is for obtaining captions for all the images in a user-input directory\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m input_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the image - directory: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m auto_caption_dir_images(input_dir)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1287\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# The following is for obtaining captions for all the images in a user-input directory\n",
    "input_dir = input(\"Enter the image - directory: \")\n",
    "df = auto_caption_dir_images(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529780d-1ef5-4346-82bc-db7e59b2fdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28992d7-95da-4ff6-9b0c-14cfead7b3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
